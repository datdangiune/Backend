{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cf8e39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ l∆∞u arbitrum_deposit_events.parquet\n",
      "‚úÖ ƒê√£ l∆∞u arbitrum_deposit_events.csv\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "\n",
    "# K·∫øt n·ªëi MongoDB\n",
    "uri = \"mongodb://dsReader:ds_reader_ndFwBkv3LsZYjtUS@178.128.85.210:27017,104.248.148.66:27017,103.253.146.224:27017/\"\n",
    "client = MongoClient(uri)\n",
    "db = client['arbitrum_blockchain_etl']  \n",
    "\n",
    "collection = db[\"lending_events\"]\n",
    "cursor = collection.find(\n",
    "    {\"event_type\": \"DEPOSIT\"},\n",
    "    {\n",
    "        \"wallet\": 1,\n",
    "        \"user\": 1,\n",
    "        \"amount\": 1,\n",
    "        \"block_timestamp\": 1,\n",
    "        \"event_type\": 1,\n",
    "        \"contract_address\": 1,\n",
    "        \"transaction_hash\": 1\n",
    "    }\n",
    ").limit(10000)  \n",
    "# Chuy·ªÉn sang DataFrame\n",
    "df = pd.DataFrame(list(cursor))\n",
    "df.drop(columns=[\"_id\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "# (T√πy ch·ªçn) Chuy·ªÉn block_timestamp sang datetime\n",
    "df[\"block_time\"] = pd.to_datetime(df[\"block_timestamp\"], unit=\"s\")\n",
    "\n",
    "# L∆∞u ra file\n",
    "df.to_parquet(\"arbitrum_deposit_events.parquet\", index=False)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ l∆∞u arbitrum_deposit_events.parquet\")\n",
    "df.to_csv(\"arbitrum_deposit_events.csv\", index=False)\n",
    "print(\"‚úÖ ƒê√£ l∆∞u arbitrum_deposit_events.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d74761f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              userName  followersCount  verified    G_score\n",
      "0      Dogearmybullish          128196     False  11.761323\n",
      "1         FlokiArmybul          143416     False  11.873512\n",
      "2           Candy_Gems          136085     False  11.821042\n",
      "3       PixelRealmlabs          205972     False  12.235500\n",
      "4         cryptoskullx          131869     False  11.789572\n",
      "...                ...             ...       ...        ...\n",
      "16808        DavidCayJ          171286     False  12.051096\n",
      "16809    ChainOpera_AI          127210     False  11.753602\n",
      "16810     RupertLowe10          179743     False  12.099289\n",
      "16811          GUSMGMT          168062     False  12.032094\n",
      "16812    AAAAA4A4AAAAA          218639     False  12.295182\n",
      "\n",
      "[16813 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# T·∫£i d·ªØ li·ªáu JSON t·ª´ file ho·∫∑c t·ª´ bi·∫øn\n",
    "file_path = r\"D:\\Backend\\data\\cdp_db.twitter_users_100000_500000.json\"  # <- n·∫øu b·∫°n l∆∞u file ri√™ng\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Chuy·ªÉn sang DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ƒê·∫£m b·∫£o c√≥ tr∆∞·ªùng followersCount v√† verified\n",
    "df = df[df[\"followersCount\"].notnull()]\n",
    "df[\"followersCount\"] = pd.to_numeric(df[\"followersCount\"], errors=\"coerce\")\n",
    "\n",
    "# Chuy·ªÉn verified th√†nh is_verified: 1 n·∫øu True, 0 n·∫øu False\n",
    "df[\"is_verified\"] = df[\"verified\"].apply(lambda x: 1 if x is True else 0)\n",
    "\n",
    "# T√≠nh ƒëi·ªÉm G\n",
    "df[\"G_score\"] = np.log1p(df[\"followersCount\"]) * (1 + 0.3 * df[\"is_verified\"])\n",
    "\n",
    "# In k·∫øt qu·∫£\n",
    "print(df[[\"userName\", \"followersCount\", \"verified\", \"G_score\"]])\n",
    "\n",
    "# N·∫øu mu·ªën l∆∞u ra file\n",
    "df.to_csv(\"twitter_users_gscore.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "024034ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== K·∫æT QU·∫¢ T√çNH ƒêI·ªÇM ===\n",
      "User: @wyckoffweb\n",
      "User ID: 1547302457432236035\n",
      "Followers: 172,082\n",
      "Verified: False\n",
      "\n",
      "üìä ƒêI·ªÇM G (Followers Influence): 12.06\n",
      "üìà ƒêI·ªÇM H (Engagement Score): 88.05\n",
      "   ‚îî‚îÄ L·∫•y 50 tweets g·∫ßn nh·∫•t (c√≥ 170 tweets g·ªëc)\n",
      "\n",
      "=== TH·ªêNG K√ä TWEETS ===\n",
      "T·ªïng tweets: 170\n",
      "Tweets g·ªëc: 170\n",
      "Retweets: 0\n",
      "\n",
      "=== CHI TI·∫æT T√çNH ƒêI·ªÇM H ===\n",
      "S·ªë tweets ph√¢n t√≠ch: 50\n",
      "T·ªïng engagement: 36,851\n",
      "Engagement trung b√¨nh (H'): 737.02\n",
      "Engagement th·∫•p nh·∫•t: 139\n",
      "Engagement cao nh·∫•t: 2649\n",
      "\n",
      "üèÜ TOP 5 TWEETS C√ì ENGAGEMENT CAO NH·∫§T:\n",
      "1. Tweet 1922969560... - Engagement: 2649\n",
      "   Likes: 835, Retweets: 550, Replies: 714\n",
      "2. Tweet 1932816396... - Engagement: 2588\n",
      "   Likes: 811, Retweets: 560, Replies: 657\n",
      "3. Tweet 1921306954... - Engagement: 1839\n",
      "   Likes: 1290, Retweets: 77, Replies: 395\n",
      "4. Tweet 1917614908... - Engagement: 1311\n",
      "   Likes: 743, Retweets: 210, Replies: 148\n",
      "5. Tweet 1927075015... - Engagement: 1186\n",
      "   Likes: 826, Retweets: 54, Replies: 252\n",
      "\n",
      "=== TH·ªêNG K√ä T·ªîNG QUAN ===\n",
      "T·ªïng likes: 71,529\n",
      "T·ªïng retweets: 19,697\n",
      "T·ªïng replies: 33,851\n",
      "Engagement trung b√¨nh (to√†n b·ªô): 851.61\n",
      "\n",
      "‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o user_scores_detailed.csv\n",
      "‚úÖ ƒê√£ l∆∞u chi ti·∫øt engagement v√†o tweets_engagement_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def calculate_g_score(followers_count: int, verified: bool) -> float:\n",
    "    \"\"\"\n",
    "    T√≠nh ƒëi·ªÉm G - Followers Influence\n",
    "    \n",
    "    ƒêI·ªÇM G = Log(1+followersCount) x (1 + 0.3 x 1_verified)\n",
    "    \"\"\"\n",
    "    verified_multiplier = 1 + (0.3 if verified else 0)\n",
    "    g_score = np.log1p(followers_count) * verified_multiplier\n",
    "    return g_score\n",
    "\n",
    "def get_n_recent_tweets(tweets_data: List[Dict[str, Any]]) -> tuple[List[Dict[str, Any]], int, str]:\n",
    "    \"\"\"\n",
    "    L·∫•y N tweets g·∫ßn nh·∫•t d·ª±a tr√™n s·ªë l∆∞·ª£ng tweets g·ªëc (kh√¥ng ph·∫£i retweets)\n",
    "    \n",
    "    Returns:\n",
    "        - Danh s√°ch N tweets g·∫ßn nh·∫•t\n",
    "        - S·ªë N ƒë∆∞·ª£c s·ª≠ d·ª•ng\n",
    "        - L√Ω do ch·ªçn N\n",
    "    \"\"\"\n",
    "    # L·ªçc ch·ªâ l·∫•y tweets g·ªëc (kh√¥ng ph·∫£i retweets)\n",
    "    original_tweets = [tweet for tweet in tweets_data if not tweet.get('retweetedTweet')]\n",
    "    \n",
    "    # S·∫Øp x·∫øp theo th·ªùi gian t·∫°o (m·ªõi nh·∫•t tr∆∞·ªõc)\n",
    "    original_tweets.sort(key=lambda x: x.get('timestamp', 0), reverse=True)\n",
    "    \n",
    "    total_original = len(original_tweets)\n",
    "    \n",
    "    # X√°c ƒë·ªãnh N d·ª±a tr√™n s·ªë l∆∞·ª£ng tweets\n",
    "    if total_original < 5:\n",
    "        return [], 0, f\"Kh√¥ng ƒë·ªß d·ªØ li·ªáu (ch·ªâ c√≥ {total_original} tweets g·ªëc, c·∫ßn t·ªëi thi·ªÉu 5)\"\n",
    "    elif 5 <= total_original < 10:\n",
    "        n = 5\n",
    "        reason = f\"L·∫•y 5 tweets g·∫ßn nh·∫•t (c√≥ {total_original} tweets g·ªëc)\"\n",
    "    elif 10 <= total_original < 30:\n",
    "        n = 10\n",
    "        reason = f\"L·∫•y 10 tweets g·∫ßn nh·∫•t (c√≥ {total_original} tweets g·ªëc)\"\n",
    "    elif 30 <= total_original < 50:\n",
    "        n = 30\n",
    "        reason = f\"L·∫•y 30 tweets g·∫ßn nh·∫•t (c√≥ {total_original} tweets g·ªëc)\"\n",
    "    else:  # >= 50\n",
    "        n = 50\n",
    "        reason = f\"L·∫•y 50 tweets g·∫ßn nh·∫•t (c√≥ {total_original} tweets g·ªëc)\"\n",
    "    \n",
    "    return original_tweets[:n], n, reason\n",
    "\n",
    "def calculate_h_score(tweets_data: List[Dict[str, Any]]) -> tuple[float, int, str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    T√≠nh ƒëi·ªÉm H - Engagement Score\n",
    "    \n",
    "    H' = (1/N) x Œ£(likes_i + 2*retweetCounts_i + replyCounts_i)\n",
    "    ƒêi·ªÉm H = 100 x H' / (H' + c) v·ªõi c=100\n",
    "    \n",
    "    Returns:\n",
    "        - H score\n",
    "        - S·ªë N tweets ƒë∆∞·ª£c s·ª≠ d·ª•ng\n",
    "        - L√Ω do ch·ªçn N\n",
    "        - Th·ªëng k√™ chi ti·∫øt\n",
    "    \"\"\"\n",
    "    # L·∫•y N tweets g·∫ßn nh·∫•t\n",
    "    recent_tweets, n, reason = get_n_recent_tweets(tweets_data)\n",
    "    \n",
    "    if n == 0:\n",
    "        return 0.0, 0, reason, {}\n",
    "    \n",
    "    total_engagement = 0\n",
    "    engagement_details = []\n",
    "    \n",
    "    for tweet in recent_tweets:\n",
    "        likes = tweet.get('likes', 0)\n",
    "        retweets = tweet.get('retweetCounts', 0)\n",
    "        replies = tweet.get('replyCounts', 0)\n",
    "        \n",
    "        # C√¥ng th·ª©c: likes + 2*retweets + replies\n",
    "        engagement = likes + (2 * retweets) + replies\n",
    "        total_engagement += engagement\n",
    "        \n",
    "        engagement_details.append({\n",
    "            'tweet_id': tweet.get('_id'),\n",
    "            'created': tweet.get('created'),\n",
    "            'likes': likes,\n",
    "            'retweets': retweets,\n",
    "            'replies': replies,\n",
    "            'engagement_score': engagement\n",
    "        })\n",
    "    \n",
    "    # T√≠nh engagement trung b√¨nh (H')\n",
    "    h_prime = total_engagement / n\n",
    "    \n",
    "    # Chu·∫©n h√≥a v·ªÅ thang 0-100 v·ªõi c=100\n",
    "    c = 100\n",
    "    h_score = 100 * h_prime / (h_prime + c)\n",
    "    \n",
    "    # Th·ªëng k√™ chi ti·∫øt\n",
    "    stats = {\n",
    "        'total_engagement': total_engagement,\n",
    "        'avg_engagement': h_prime,\n",
    "        'tweets_analyzed': n,\n",
    "        'min_engagement': min(detail['engagement_score'] for detail in engagement_details),\n",
    "        'max_engagement': max(detail['engagement_score'] for detail in engagement_details),\n",
    "        'engagement_details': engagement_details\n",
    "    }\n",
    "    \n",
    "    return h_score, n, reason, stats\n",
    "\n",
    "def process_user_data(user_profile_file: str, user_tweets_file: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    X·ª≠ l√Ω d·ªØ li·ªáu v√† t√≠nh ƒëi·ªÉm G, H cho user\n",
    "    \"\"\"\n",
    "    # ƒê·ªçc d·ªØ li·ªáu profile\n",
    "    with open(user_profile_file, 'r', encoding='utf-8') as f:\n",
    "        user_profile = json.load(f)[0]  # L·∫•y user ƒë·∫ßu ti√™n\n",
    "    \n",
    "    # ƒê·ªçc d·ªØ li·ªáu tweets\n",
    "    with open(user_tweets_file, 'r', encoding='utf-8') as f:\n",
    "        tweets_data = json.load(f)\n",
    "    \n",
    "    # T√≠nh ƒëi·ªÉm G\n",
    "    followers_count = user_profile.get('followersCount', 0)\n",
    "    verified = user_profile.get('verified', False)\n",
    "    g_score = calculate_g_score(followers_count, verified)\n",
    "    \n",
    "    # T√≠nh ƒëi·ªÉm H\n",
    "    h_score, n_analyzed, h_reason, h_stats = calculate_h_score(tweets_data)\n",
    "    \n",
    "    # Th·ªëng k√™ t·ªïng quan\n",
    "    total_tweets = len(tweets_data)\n",
    "    original_tweets = len([t for t in tweets_data if not t.get('retweetedTweet')])\n",
    "    retweets = total_tweets - original_tweets\n",
    "    \n",
    "    total_likes = sum(t.get('likes', 0) for t in tweets_data)\n",
    "    total_retweets = sum(t.get('retweetCounts', 0) for t in tweets_data)\n",
    "    total_replies = sum(t.get('replyCounts', 0) for t in tweets_data)\n",
    "    \n",
    "    return {\n",
    "        'user_id': user_profile.get('_id'),\n",
    "        'username': user_profile.get('userName'),\n",
    "        'followers_count': followers_count,\n",
    "        'verified': verified,\n",
    "        'g_score': round(g_score, 2),\n",
    "        'h_score': round(h_score, 2),\n",
    "        'h_calculation_reason': h_reason,\n",
    "        'tweets_analyzed_for_h': n_analyzed,\n",
    "        'h_stats': h_stats,\n",
    "        'total_tweets': total_tweets,\n",
    "        'original_tweets': original_tweets,\n",
    "        'retweets': retweets,\n",
    "        'total_likes': total_likes,\n",
    "        'total_retweets': total_retweets,\n",
    "        'total_replies': total_replies,\n",
    "        'avg_engagement_all_tweets': round((total_likes + 2*total_retweets + total_replies) / max(original_tweets, 1), 2)\n",
    "    }\n",
    "\n",
    "def print_detailed_analysis(result: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    In k·∫øt qu·∫£ ph√¢n t√≠ch chi ti·∫øt\n",
    "    \"\"\"\n",
    "    print(\"=== K·∫æT QU·∫¢ T√çNH ƒêI·ªÇM ===\")\n",
    "    print(f\"User: @{result['username']}\")\n",
    "    print(f\"User ID: {result['user_id']}\")\n",
    "    print(f\"Followers: {result['followers_count']:,}\")\n",
    "    print(f\"Verified: {result['verified']}\")\n",
    "    print(f\"\")\n",
    "    print(f\"üìä ƒêI·ªÇM G (Followers Influence): {result['g_score']}\")\n",
    "    print(f\"üìà ƒêI·ªÇM H (Engagement Score): {result['h_score']}\")\n",
    "    print(f\"   ‚îî‚îÄ {result['h_calculation_reason']}\")\n",
    "    print(f\"\")\n",
    "    print(\"=== TH·ªêNG K√ä TWEETS ===\")\n",
    "    print(f\"T·ªïng tweets: {result['total_tweets']}\")\n",
    "    print(f\"Tweets g·ªëc: {result['original_tweets']}\")\n",
    "    print(f\"Retweets: {result['retweets']}\")\n",
    "    print(f\"\")\n",
    "    print(\"=== CHI TI·∫æT T√çNH ƒêI·ªÇM H ===\")\n",
    "    if result['tweets_analyzed_for_h'] > 0:\n",
    "        h_stats = result['h_stats']\n",
    "        print(f\"S·ªë tweets ph√¢n t√≠ch: {result['tweets_analyzed_for_h']}\")\n",
    "        print(f\"T·ªïng engagement: {h_stats['total_engagement']:,}\")\n",
    "        print(f\"Engagement trung b√¨nh (H'): {h_stats['avg_engagement']:.2f}\")\n",
    "        print(f\"Engagement th·∫•p nh·∫•t: {h_stats['min_engagement']}\")\n",
    "        print(f\"Engagement cao nh·∫•t: {h_stats['max_engagement']}\")\n",
    "        \n",
    "        # Hi·ªÉn th·ªã top 5 tweets c√≥ engagement cao nh·∫•t\n",
    "        top_tweets = sorted(h_stats['engagement_details'], \n",
    "                          key=lambda x: x['engagement_score'], reverse=True)[:5]\n",
    "        print(f\"\\nüèÜ TOP 5 TWEETS C√ì ENGAGEMENT CAO NH·∫§T:\")\n",
    "        for i, tweet in enumerate(top_tweets, 1):\n",
    "            print(f\"{i}. Tweet {tweet['tweet_id'][:10]}... - Engagement: {tweet['engagement_score']}\")\n",
    "            print(f\"   Likes: {tweet['likes']}, Retweets: {tweet['retweets']}, Replies: {tweet['replies']}\")\n",
    "    else:\n",
    "        print(\"‚ùå Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ t√≠nh ƒëi·ªÉm H\")\n",
    "    \n",
    "    print(f\"\\n=== TH·ªêNG K√ä T·ªîNG QUAN ===\")\n",
    "    print(f\"T·ªïng likes: {result['total_likes']:,}\")\n",
    "    print(f\"T·ªïng retweets: {result['total_retweets']:,}\")\n",
    "    print(f\"T·ªïng replies: {result['total_replies']:,}\")\n",
    "    print(f\"Engagement trung b√¨nh (to√†n b·ªô): {result['avg_engagement_all_tweets']}\")\n",
    "\n",
    "# S·ª≠ d·ª•ng code\n",
    "if __name__ == \"__main__\":\n",
    "    # ƒê∆∞·ªùng d·∫´n t·ªõi files\n",
    "    user_profile_file = \"user_profile_1547302457432236035.json\"  # File ch·ª©a th√¥ng tin user\n",
    "    user_tweets_file = \"user_1547302457432236035.json\"          # File ch·ª©a tweets c·ªßa user\n",
    "    \n",
    "    try:\n",
    "        result = process_user_data(user_profile_file, user_tweets_file)\n",
    "        \n",
    "        # In k·∫øt qu·∫£ chi ti·∫øt\n",
    "        print_detailed_analysis(result)\n",
    "        \n",
    "        # L∆∞u k·∫øt qu·∫£ ra file CSV\n",
    "        output_data = {\n",
    "            'user_id': result['user_id'],\n",
    "            'username': result['username'],\n",
    "            'followers_count': result['followers_count'],\n",
    "            'verified': result['verified'],\n",
    "            'g_score': result['g_score'],\n",
    "            'h_score': result['h_score'],\n",
    "            'tweets_analyzed_for_h': result['tweets_analyzed_for_h'],\n",
    "            'h_calculation_reason': result['h_calculation_reason'],\n",
    "            'total_tweets': result['total_tweets'],\n",
    "            'original_tweets': result['original_tweets']\n",
    "        }\n",
    "        \n",
    "        output_df = pd.DataFrame([output_data])\n",
    "        output_df.to_csv(\"user_scores_detailed.csv\", index=False)\n",
    "        print(f\"\\n‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o user_scores_detailed.csv\")\n",
    "        \n",
    "        # L∆∞u chi ti·∫øt engagement c·ªßa t·ª´ng tweet ƒë∆∞·ª£c ph√¢n t√≠ch\n",
    "        if result['tweets_analyzed_for_h'] > 0:\n",
    "            engagement_df = pd.DataFrame(result['h_stats']['engagement_details'])\n",
    "            engagement_df.to_csv(\"tweets_engagement_analysis.csv\", index=False)\n",
    "            print(f\"‚úÖ ƒê√£ l∆∞u chi ti·∫øt engagement v√†o tweets_engagement_analysis.csv\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y file: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói: {e}\")\n",
    "\n",
    "# H√†m test v·ªõi data m·∫´u\n",
    "def test_n_logic():\n",
    "    \"\"\"\n",
    "    Test logic ch·ªçn N v·ªõi c√°c tr∆∞·ªùng h·ª£p kh√°c nhau\n",
    "    \"\"\"\n",
    "    print(\"=== TEST LOGIC CH·ªåN N ===\")\n",
    "    \n",
    "    test_cases = [3, 7, 15, 35, 80]\n",
    "    \n",
    "    for case in test_cases:\n",
    "        # T·∫°o fake tweets\n",
    "        fake_tweets = []\n",
    "        for i in range(case):\n",
    "            fake_tweets.append({\n",
    "                '_id': f'tweet_{i}',\n",
    "                'timestamp': 1742047202 + i,\n",
    "                'likes': 10,\n",
    "                'retweetCounts': 5,\n",
    "                'replyCounts': 2\n",
    "            })\n",
    "        \n",
    "        recent_tweets, n, reason = get_n_recent_tweets(fake_tweets)\n",
    "        print(f\"C√≥ {case} tweets g·ªëc ‚Üí {reason} ‚Üí N = {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1920f2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ K·∫æT QU·∫¢ SENTIMENT\n",
      "ƒêi·ªÉm I: 73.0/100\n",
      "S·ªë tweet ph√¢n t√≠ch: 50\n",
      "L√Ω do ch·ªçn N: D√πng 50 tweets g·∫ßn nh·∫•t\n",
      "\n",
      "M·ªôt s·ªë v√≠ d·ª•:\n",
      "1. T√çCH C·ª∞C (1): Giving away 10x @DustedBob Testnet NFT on Monad trading at 14 MON currently.\n",
      "\n",
      "‚úÖ How to join \n",
      "- Follo\n",
      "   L√Ω do: ƒêo·∫°n tweet th·ªÉ hi·ªán c·∫£m x√∫c t√≠ch c·ª±c th√¥ng qua vi·ªác t·∫∑ng qu√† NFT. Vi·ªác t·∫∑ng 10 NFT c·ªßa DustedBob Testnet v·ªõi gi√° tr·ªã giao d·ªãch hi·ªán t·∫°i l√† 14 MON cho th·∫•y s·ª± h√†o ph√≥ng v√† khuy·∫øn kh√≠ch s·ª± tham gia c·ªßa c·ªông ƒë·ªìng.  C√°c h∆∞·ªõng d·∫´n tham gia ƒë∆°n gi·∫£n (follow, like, retweet, comment) c≈©ng t·∫°o c·∫£m gi√°c d·ªÖ d√†ng v√† th√¢n thi·ªán.  T·ªïng th·ªÉ, tweet t·∫°o ra s·ª± ph·∫•n kh√≠ch v√† mong ƒë·ª£i t√≠ch c·ª±c t·ª´ ng∆∞·ªùi d√πng.\n",
      "\n",
      "2. T√çCH C·ª∞C (1): A life-changing airdrop is the ultimate reward for showing up daily.\n",
      "\n",
      "Some projects give dust.\n",
      "Some \n",
      "   L√Ω do: ƒêo·∫°n vƒÉn th·ªÉ hi·ªán m·ªôt th√°i ƒë·ªô t√≠ch c·ª±c v√† l·∫°c quan m·∫∑c d√π c√≥ s·ª± th·ª´a nh·∫≠n v·ªÅ r·ªßi ro.  T√°c gi·∫£ nh·∫•n m·∫°nh t·∫ßm quan tr·ªçng c·ªßa s·ª± ki√™n tr√¨ v√† tham gia li√™n t·ª•c trong lƒ©nh v·ª±c crypto/DeFi ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c ph·∫ßn th∆∞·ªüng l·ªõn.  M·∫∑c d√π th·ª´a nh·∫≠n s·ª± th·∫•t v·ªçng tr∆∞·ªõc nh·ªØng airdrop nh·ªè (Layeredge), t√°c gi·∫£ t·∫≠p trung v√†o tri·ªÉn v·ªçng t√≠ch c·ª±c v·ªÅ m·ªôt airdrop l·ªõn trong t∆∞∆°ng lai (nh·∫•n m·∫°nh v√†o Monad nh∆∞ m·ªôt v√≠ d·ª• ti·ªÅm nƒÉng).  Th√¥ng ƒëi·ªáp ch√≠nh l√† s·ª± ki√™n tr√¨ s·∫Ω ƒë∆∞·ª£c ƒë·ªÅn ƒë√°p, khuy·∫øn kh√≠ch ng∆∞·ªùi ƒë·ªçc ti·∫øp t·ª•c tham gia v√† kh√¥ng b·ªè cu·ªôc.  T·ª´ ng·ªØ nh∆∞ \"life-changing\", \"ultimate reward\", \"makes the years of effort make sense\" ƒë·ªÅu mang s·∫Øc th√°i t√≠ch c·ª±c, t·∫°o n√™n t·ªïng th·ªÉ l·∫°c quan c·ªßa ƒëo·∫°n vƒÉn.\n",
      "\n",
      "3. T√çCH C·ª∞C (1): A new era of prediction markets is coming to Monad and this one actually works.\n",
      "\n",
      "@opinionlabsxyz is \n",
      "   L√Ω do: ƒêo·∫°n vƒÉn th·ªÉ hi·ªán s·ª± l·∫°c quan v√† ph·∫•n kh√≠ch v·ªÅ vi·ªác ra m·∫Øt m·ªôt th·ªã tr∆∞·ªùng d·ª± ƒëo√°n m·ªõi tr√™n Monad.  C√°c ƒëi·ªÉm nh·∫•n m·∫°nh nh∆∞ \"th·ª±c s·ª± ho·∫°t ƒë·ªông\", \"kh√¥ng c·∫ßn s·ª± cho ph√©p\", \"AI-powered\", \"hi·ªáu qu·∫£\", \"b·∫•t c·ª© ai c≈©ng c√≥ th·ªÉ tham gia\", v√† \"s·∫µn s√†ng cho s·ªë ƒë√¥ng\" ƒë·ªÅu mang t√≠nh t√≠ch c·ª±c.  C√¢u cu·ªëi c√πng \"Don't fade it\" (ƒê·ª´ng b·ªè l·ª° n√≥) c·ªßng c·ªë th√™m sentiment t√≠ch c·ª±c, ng·ª• √Ω khuy·∫øn kh√≠ch ng∆∞·ªùi ƒë·ªçc tham gia v√† tin t∆∞·ªüng v√†o d·ª± √°n.  Kh√¥ng c√≥ b·∫•t k·ª≥ y·∫øu t·ªë ti√™u c·ª±c n√†o ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p.\n",
      "\n",
      "4. T√çCH C·ª∞C (1): Boundless testnet is fast approaching.\n",
      "\n",
      "Like I've said before, @boundless_xyz is doing something sim\n",
      "   L√Ω do: ƒêo·∫°n vƒÉn th·ªÉ hi·ªán s·ª± ph·∫•n kh√≠ch v√† l·∫°c quan v·ªÅ Boundless testnet s·∫Øp ra m·∫Øt.  Vi·ªác testnet m·ªü c·ª≠a cho t·∫•t c·∫£ m·ªçi ng∆∞·ªùi (kh√¥ng gi·ªëng nh∆∞ Succinct) ƒë∆∞·ª£c xem l√† m·ªôt ƒëi·ªÉm t√≠ch c·ª±c, thu h√∫t s·ª± ch√∫ √Ω v√† t·∫°o ra k·ª≥ v·ªçng t√≠ch c·ª±c v·ªÅ d·ª± √°n.  T·ª´ ng·ªØ \"fast approaching\" v√† vi·ªác so s√°nh v·ªõi Succinct m·ªôt c√°ch c√≥ l·ª£i cho Boundless ƒë·ªÅu g√≥p ph·∫ßn t·∫°o n√™n s·∫Øc th√°i t√≠ch c·ª±c.\n",
      "\n",
      "5. T√çCH C·ª∞C (1): I‚Äôm a Billionaire!\n",
      "\n",
      "I have 2 WL for the upcoming Billions Genesis¬†free NFT mint!\n",
      "\n",
      "Requirements:\n",
      "- Fo\n",
      "   L√Ω do: ƒêo·∫°n vƒÉn th·ªÉ hi·ªán c·∫£m x√∫c c·ª±c k·ª≥ t√≠ch c·ª±c.  C√¢u \"I‚Äôm a Billionaire!\" th·ªÉ hi·ªán s·ª± ph·∫•n kh√≠ch v√† t·ª± h√†o t·ªôt ƒë·ªô. Vi·ªác s·ªü h·ªØu 2 WL (Whitelist) cho ƒë·ª£t mint NFT mi·ªÖn ph√≠ c·ªßa Billions Genesis c≈©ng cho th·∫•y ng∆∞·ªùi vi·∫øt ƒëang r·∫•t l·∫°c quan v·ªÅ ti·ªÅm nƒÉng sinh l·ªùi t·ª´ d·ª± √°n n√†y.  H√†nh ƒë·ªông chia s·∫ª th√¥ng tin v·ªÅ c√°ch th·ª©c tham gia ch∆∞∆°ng tr√¨nh whitelist c√†ng c·ªßng c·ªë th√™m s·ª± t√≠ch c·ª±c, th·ªÉ hi·ªán mong mu·ªën ng∆∞·ªùi kh√°c c≈©ng c√≥ c∆° h·ªôi nh·∫≠n ƒë∆∞·ª£c NFT mi·ªÖn ph√≠.  T√≥m l·∫°i, to√†n b·ªô th√¥ng ƒëi·ªáp to√°t l√™n v·∫ª h√†o h·ª©ng v√† k·ª≥ v·ªçng cao ƒë·ªô v√†o l·ª£i √≠ch t√†i ch√≠nh t·ª´ vi·ªác s·ªü h·ªØu NFT.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Thi·∫øt l·∫≠p Gemini model\n",
    "def load_gemini(api_key):\n",
    "    genai.configure(api_key=api_key)\n",
    "    return genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "# L·ªçc N tweets g·ªëc g·∫ßn nh·∫•t\n",
    "def get_recent_original_tweets(tweets):\n",
    "    original = [t for t in tweets if not t.get(\"retweetedTweet\")]\n",
    "    original.sort(key=lambda x: x.get(\"timestamp\", 0), reverse=True)\n",
    "    total = len(original)\n",
    "    if total < 5:\n",
    "        return [], 0, \"Kh√¥ng ƒë·ªß tweets (c·∫ßn ‚â•5 tweet g·ªëc)\"\n",
    "    elif total < 10:\n",
    "        return original[:5], 5, \"D√πng 5 tweets g·∫ßn nh·∫•t\"\n",
    "    elif total < 30:\n",
    "        return original[:10], 10, \"D√πng 10 tweets g·∫ßn nh·∫•t\"\n",
    "    elif total < 50:\n",
    "        return original[:30], 30, \"D√πng 30 tweets g·∫ßn nh·∫•t\"\n",
    "    else:\n",
    "        return original[:50], 50, \"D√πng 50 tweets g·∫ßn nh·∫•t\"\n",
    "\n",
    "# G·ª≠i prompt ƒë·∫øn Gemini ƒë·ªÉ ph√¢n t√≠ch c·∫£m x√∫c\n",
    "def get_sentiment(model, text):\n",
    "    prompt = f\"\"\"\n",
    "Ph√¢n t√≠ch c·∫£m x√∫c ƒëo·∫°n sau trong lƒ©nh v·ª±c crypto/DeFi:\n",
    "\"{text}\"\n",
    "\n",
    "Tr·∫£ v·ªÅ JSON:\n",
    "{{\n",
    "  \"sentiment_score\": -1 | 0 | 1,\n",
    "  \"sentiment_label\": \"TI√äU C·ª∞C\" | \"TRUNG L·∫¨P\" | \"T√çCH C·ª∞C\",\n",
    "  \"explanation\": \"...\",\n",
    "  \"confidence\": 0.1 - 1.0\n",
    "}}\"\"\"\n",
    "    try:\n",
    "        response = model.generate_content(prompt).text\n",
    "        response = response.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        return json.loads(response)\n",
    "    except Exception:\n",
    "        return {\n",
    "            \"sentiment_score\": 0,\n",
    "            \"sentiment_label\": \"TRUNG L·∫¨P\",\n",
    "            \"explanation\": \"L·ªói x·ª≠ l√Ω\",\n",
    "            \"confidence\": 0.1\n",
    "        }\n",
    "\n",
    "# T√≠nh ƒëi·ªÉm I sentiment\n",
    "def compute_sentiment_score(tweets, model):\n",
    "    recent, n, reason = get_recent_original_tweets(tweets)\n",
    "    if n == 0:\n",
    "        return 50.0, 0, reason, []\n",
    "\n",
    "    total = 0\n",
    "    results = []\n",
    "\n",
    "    for tweet in recent:\n",
    "        res = get_sentiment(model, tweet.get(\"text\", \"\"))\n",
    "        total += res[\"sentiment_score\"]\n",
    "        results.append({\n",
    "            \"id\": tweet.get(\"_id\"),\n",
    "            \"text\": tweet.get(\"text\", \"\")[:100],\n",
    "            **res\n",
    "        })\n",
    "        time.sleep(1)  # tr√°nh b·ªã gi·ªõi h·∫°n\n",
    "\n",
    "    avg = total / len(results)\n",
    "    i_score = round(50 * (1 + avg), 2)\n",
    "    return i_score, len(results), reason, results\n",
    "\n",
    "# H√†m ch√≠nh\n",
    "def run_sentiment_analysis(input_file, api_key):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        tweets = json.load(f)\n",
    "\n",
    "    model = load_gemini(api_key)\n",
    "    score, n, reason, details = compute_sentiment_score(tweets, model)\n",
    "\n",
    "    print(\"üéØ K·∫æT QU·∫¢ SENTIMENT\")\n",
    "    print(f\"ƒêi·ªÉm I: {score}/100\")\n",
    "    print(f\"S·ªë tweet ph√¢n t√≠ch: {n}\")\n",
    "    print(f\"L√Ω do ch·ªçn N: {reason}\")\n",
    "    print(\"\\nM·ªôt s·ªë v√≠ d·ª•:\")\n",
    "    for i, d in enumerate(details[:5], 1):\n",
    "        print(f\"{i}. {d['sentiment_label']} ({d['sentiment_score']}): {d['text']}\")\n",
    "        print(f\"   L√Ω do: {d['explanation']}\")\n",
    "        print()\n",
    "\n",
    "# G·ªçi ch∆∞∆°ng tr√¨nh\n",
    "if __name__ == \"__main__\":\n",
    "    GEMINI_KEY = \"AIzaSyACb8UfQyc4Jqz8pbz9qjT_HiTPfx-Ea6I\"\n",
    "    INPUT_FILE = \"user_1547302457432236035.json\"\n",
    "    run_sentiment_analysis(INPUT_FILE, GEMINI_KEY)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
